# Littoral Pipeline: GeoJSON Conversion and Public Data Infrastructure

## Overview

This document summarizes the complete implementation of GeoJSON conversion capabilities and public data serving infrastructure for the Littoral Pipeline. The enhanced system now includes a 14th pipeline step that converts processed shoreline data to publicly accessible GeoJSON format with comprehensive metadata storage and API access.

## ğŸ—ï¸ Infrastructure Components

### 1. Google Cloud Storage Buckets
- **littoral-geojson**: Stores processed GeoJSON files from pipeline
- **littoral-metadata-staging**: Staging area for metadata processing
- **littoral-public-data**: Public bucket with CORS enabled for direct access
  - Public read permissions for all users
  - CORS configuration for web browser access

### 2. BigQuery Data Warehouse
- **Project**: useful-theory-442820-q8
- **Dataset**: shoreline_metadata
- **Table**: shoreline_data (13 fields)
  - `shoreline_id`: Unique identifier
  - `site_name`: Processing site name
  - `image_date`: Satellite image acquisition date
  - `processing_date`: Pipeline execution timestamp
  - `geojson_url`: Public URL to GeoJSON file
  - `geometry`: Bounding box as GEOGRAPHY type
  - `total_length_m`: Total shoreline length in meters
  - `num_features`: Number of vector features
  - `avg_feature_length_m`: Average feature length
  - `quality_score`: Computed quality metric
  - `data_source`: Source identifier ("littoral_pipeline")
  - `processing_version`: Pipeline version number
  - `metadata`: JSON blob with additional processing metadata

### 3. Cloud Run API Service
- **Service**: littoral-metadata-api
- **URL**: https://littoral-metadata-api-918741372271.us-central1.run.app
- **Features**: RESTful API for querying shoreline metadata
- **Scaling**: Auto-scaling from 0 to 100 instances
- **Authentication**: Public access (no authentication required)

### 4. Container Registry
- **Repository**: us-central1-docker.pkg.dev/useful-theory-442820-q8/littoral-pipeline
- **Image**: metadata-api:latest
- **Base**: Python 3.11 slim with Flask + Google Cloud libraries

## ğŸ”„ Pipeline Enhancement

### New Step 14: GeoJSON Conversion

**Class**: `GeoJSONConversion` (in `pipeline/pipeline_advanced.py`)

**Functionality**:
1. **Input Processing**: Accepts tidal-corrected shoreline files (`.gpkg` format)
2. **Format Conversion**: Converts to web-compatible GeoJSON (EPSG:4326)
3. **Metadata Enrichment**: Adds site info, processing dates, and data source
4. **Cloud Upload**: Uploads to both processing and public buckets
5. **BigQuery Integration**: Stores comprehensive metadata for querying
6. **Quality Metrics**: Computes length, feature count, and quality scores

**Integration Points**:
- Added to `PipelineOrchestrator` step definitions
- Integrated with test framework (`GeoJSONTester`)
- Updated test configurations to include step 14

## ğŸ“Š Test Framework Updates

### Enhanced Test Coverage
- **New Tester**: `GeoJSONTester` class for comprehensive validation
- **Metrics Collected**:
  - Conversion success rate
  - Processing time per file
  - BigQuery upload success
  - Public URL accessibility
- **Test Configurations**: Updated all YAML configs to include GeoJSON step

### Test Execution
```bash
cd test_framework
python test_runner.py config/quick_smoke_test.yaml
```

## ğŸŒ API Endpoints

### Base URL
```
https://littoral-metadata-api-918741372271.us-central1.run.app
```

### Available Endpoints

#### 1. Health Check
```
GET /health
```
Returns service status and timestamp.

#### 2. List Sites
```
GET /api/sites
```
Returns all available sites with summary statistics.

#### 3. Query Shorelines
```
GET /api/shorelines?site=Fenfushi&start_date=2023-01-01&min_quality=0.5
```
Query parameters:
- `site`: Filter by site name
- `start_date`, `end_date`: Date range filter
- `min_quality`: Minimum quality score
- `limit`, `offset`: Pagination

#### 4. Shoreline Details
```
GET /api/shorelines/{shoreline_id}
```
Get detailed information for a specific shoreline.

#### 5. Dataset Statistics
```
GET /api/stats
```
Overall dataset statistics and summary metrics.

## ğŸ”§ Usage Examples

### Direct GeoJSON Access
```javascript
// Direct browser access to public GeoJSON
fetch('https://storage.googleapis.com/littoral-public-data/shorelines/Fenfushi_20230415.geojson')
  .then(response => response.json())
  .then(data => console.log('Shoreline data:', data));
```

### API Query Examples
```bash
# Get all sites
curl "https://littoral-metadata-api-918741372271.us-central1.run.app/api/sites"

# Get recent shorelines for a site
curl "https://littoral-metadata-api-918741372271.us-central1.run.app/api/shorelines?site=Fenfushi&start_date=2023-01-01"

# Get dataset statistics
curl "https://littoral-metadata-api-918741372271.us-central1.run.app/api/stats"
```

### Pipeline Integration
```python
# Run full pipeline with GeoJSON conversion
from pipeline import PipelineOrchestrator, PipelineConfig

config = PipelineConfig('config/Fenfushi.json')
orchestrator = PipelineOrchestrator(config)

# This now includes step 14: GeoJSON conversion
results = orchestrator.run_full_pipeline()

# Access conversion results
geojson_results = results['geojson_convert']
print(f"Converted {geojson_results['converted_count']} files")
print(f"Public URLs: {geojson_results['public_urls']}")
```

## ğŸ“ File Structure

```
cloud_services/
â”œâ”€â”€ metadata_api.py          # Flask API application
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Container definition
â””â”€â”€ app.yaml                # Cloud Run configuration

pipeline/
â”œâ”€â”€ pipeline_advanced.py    # Contains GeoJSONConversion class
â””â”€â”€ pipeline_orchestrator.py # Updated with step 14

test_framework/
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ step_testers.py     # Contains GeoJSONTester class
â””â”€â”€ config/
    â””â”€â”€ *.yaml              # Updated test configurations
```

## ğŸš€ Deployment Process

### 1. Build and Deploy API
```bash
cd cloud_services
gcloud builds submit --tag us-central1-docker.pkg.dev/useful-theory-442820-q8/littoral-pipeline/metadata-api
gcloud run deploy littoral-metadata-api --image us-central1-docker.pkg.dev/useful-theory-442820-q8/littoral-pipeline/metadata-api --region=us-central1 --allow-unauthenticated
```

### 2. Pipeline Execution
```bash
cd Littoral_S2download
python littoral_pipeline.py --config config/site_config.json
```

### 3. Test Validation
```bash
cd test_framework
python test_runner.py config/performance_benchmark.yaml
```

## ğŸ” Monitoring and Maintenance

### Cloud Run Metrics
- Monitor through Google Cloud Console
- Auto-scaling based on request volume
- Error tracking and logging

### BigQuery Monitoring
- Table size and growth tracking
- Query performance optimization
- Data quality validation

### Storage Monitoring
- Bucket usage and costs
- Public access patterns
- Data lifecycle management

## ğŸ“ˆ Performance Characteristics

### Pipeline Step 14 Metrics
- **Average processing time**: ~2-5 seconds per file
- **Memory usage**: <100MB peak
- **Network transfer**: ~500KB per GeoJSON file
- **BigQuery insert rate**: ~10 records/second

### API Performance
- **Cold start**: <10 seconds
- **Warm response**: <200ms
- **Concurrent users**: Up to 100 (auto-scaling)
- **BigQuery query time**: <1 second for most queries

## ğŸ¯ Next Steps and Recommendations

### Short Term
1. **API Authentication**: Implement API keys for usage tracking
2. **Caching**: Add Redis for frequently accessed data
3. **Monitoring**: Set up alerting for service health

### Medium Term
1. **Data Validation**: Enhanced quality checks before BigQuery insert
2. **Batch Processing**: Optimize for large-scale shoreline uploads
3. **Web Interface**: Build dashboard for data exploration

### Long Term
1. **Multi-region**: Deploy API to multiple regions for global access
2. **Data Pipeline**: Automated data quality and anomaly detection
3. **Integration**: APIs for external systems and partners

## ğŸ›¡ï¸ Security and Access Control

### Current Configuration
- **Public Data**: Read-only access to GeoJSON files
- **API Access**: Unauthenticated public access
- **BigQuery**: Service account with minimal permissions

### Security Best Practices
- Service accounts with least privilege
- CORS configuration for browser security
- Regular security updates for container images
- Network policies for internal services

---

**Status**: âœ… **Deployment Complete**
**Last Updated**: October 20, 2025
**Contact**: Littoral Pipeline Team